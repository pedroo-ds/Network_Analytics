{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de dados de mídia social"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coletando os dados!\n",
    "\n",
    "Para conseguir acesso a API do twitter para coleta de posts, é necessário seguir os passos conforme o site: https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a autenticação do pacote tweepy\n",
    "\n",
    "# Carregando os pacotes necessários\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "# Autenticação da \"consumer key\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\" \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# Autenticação da \"access key\" \n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Configurando a API com a autenticação\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as palavras ou hashtags que serão rastreadas\n",
    "keywords_to_track = ['Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coletando posts do twitter através da API\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "path = '/packages/Lib/site-packages'\n",
    "sys.path.insert(0, os.getcwd() + path)\n",
    "\n",
    "# Criando o SListener\n",
    "class SListener(StreamListener):\n",
    "    def __init__(self, api = None, fprefix = 'streamer'):\n",
    "        self.api = api or API()\n",
    "        self.counter = 0\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if 'in_reply_to_status' in data:\n",
    "            self.on_status(data)\n",
    "        elif 'delete' in data:\n",
    "            delete = json.loads(data)['delete']['status']\n",
    "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
    "                return False\n",
    "        elif 'limit' in data:\n",
    "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
    "                return False\n",
    "        elif 'warning' in data:\n",
    "            warning = json.loads(data)['warnings']\n",
    "            print(\"WARNING: %s\" % warning['message'])\n",
    "            return\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.output.write(status)\n",
    "        self.counter += 1\n",
    "        if self.counter >= 20000 # define quantos posts serão coletados, no caso são 20.000 posts\n",
    "            self.output.close()\n",
    "            self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "            self.counter = 0\n",
    "        return\n",
    "\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "    def on_limit(self, track):\n",
    "        print(\"WARNING: Limitation notice received, tweets missed: %d\" % track)\n",
    "        return\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        return \n",
    "\n",
    "    def on_timeout(self):\n",
    "        print(\"Timeout, sleeping for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        return \n",
    "\n",
    "# Criando uma instância para o SListener \n",
    "listen = SListener(api, 'Trump') # Inclui a palavra Trump no nome do arquivo\n",
    "\n",
    "# Criando uma instância para Stream\n",
    "stream = Stream(auth, listen)\n",
    "\n",
    "# Coletando os posts do twitter!\n",
    "print('A coleta dos posts começou! Obs: para pausar a coleta de posts do twitter através do stream, vá na aba \"Files\" do jupyter notebook, selecione o arquivo do jupyter notebook e clique em \"Shutdown\", isso é necessário para os códigos adiante funcionarem!')\n",
    "stream.filter(track = keywords_to_track, languages=['en']) # o argumento \"languages\" define a língua dos posts que serão coletados no twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregando os pacotes para a transformação dos arquivo(s) .json para data frame do pandas\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando o nome dos arquivos .json que estão no diretório\n",
    "files  = list(glob.iglob('*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntando os arquivos .json \n",
    "for f in files:\n",
    "    fh = open(f, 'r', encoding = 'utf-8')\n",
    "    tweets_json = fh.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo linhas vazias\n",
    "tweets_json = list(filter(len, tweets_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que irá armazenar todos os posts do twitter \n",
    "tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando pacote para ler os arquivos .json\n",
    "import json\n",
    "\n",
    "# Iterando através de cada tweet\n",
    "for tweet in tweets_json:\n",
    "    try:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "    \n",
    "        # Armazenando o nome do usuário\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "    \n",
    "        # Armazenando o nome do usuário que fez retweet e armazenando o texto \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']\n",
    "        \n",
    "        # Armazenando citações\n",
    "        if 'quoted_status' in tweet_obj:\n",
    "            tweet_obj['quoted_status-text'] = tweet_obj['quoted_status']['text'] \n",
    "            tweet_obj['quoted_status-user-screen_name'] = tweet_obj['quoted_status']['user']['screen_name']\n",
    "    \n",
    "        tweets.append(tweet_obj)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criando o data frame com pandas!\n",
    "df_tweet = pd.DataFrame(tweets)\n",
    "df_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimento das postagens\n",
    "\n",
    "A análise de sentimento é uma ferramenta de classificação de texto que analisa as palavras e retorna um sentimento positivo, negativo ou neutro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Criando uma instância para a função SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Gerando scores de sentimento\n",
    "sentiment = df_tweet['text'].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluindo os scores de sentimento no data frame\n",
    "s = []\n",
    "\n",
    "for index, i in enumerate(sentiment):\n",
    "    s.append(sentiment[index].get('compound'))\n",
    "\n",
    "df_tweet['sentiment'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de posts positivos e negativos\n",
    "print(\"Posts positivos:\" + str(sum(df_tweet['sentiment'] > 0.6)))\n",
    "print(\"Posts negativos:\" + str(sum(df_tweet['sentiment'] < -0.6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de rede\n",
    "\n",
    "### Criando a rede de retweets\n",
    "\n",
    "A mídia social é por natureza uma rede. As redes do Twitter se manifestam de várias maneiras, um dos tipos mais importantes de redes que aparecem no Twitter são as redes de retweets. Podemos representá-los como gráficos direcionados, com o usuário retweetando como a fonte e a pessoa retweetada como o alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Criando a rede de retweets a partir da edgelist do data frame\n",
    "G_rt = nx.from_pandas_edgelist(\n",
    "    df_tweet,\n",
    "    source = 'user-screen_name', \n",
    "    target = 'retweeted_status-user-screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    "    \n",
    "# Visualizando o número de nós\n",
    "print('Nós:', len(G_rt.nodes()))\n",
    "\n",
    "# Visualizando o número de arestas\n",
    "print('Arestas:', len(G_rt.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a rede de resposta\n",
    "\n",
    "Enquanto a rede de retweets sinaliza concordância, a rede de resposta pode sinalizar discussão, deliberação e discordância. As propriedades de rede são as mesmas, no entanto: a rede é direcionada, a origem é a resposta e o destino é o usuário que está sendo respondido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a rede de resposta da edgelist\n",
    "G_reply = nx.from_pandas_edgelist(\n",
    "    df_tweet,\n",
    "    source = 'user-screen_name', \n",
    "    target = 'in_reply_to_screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    "    \n",
    "# Visualizando o número de nós\n",
    "print('Nós:', len(G_reply.nodes()))\n",
    "\n",
    "# Visualizando o número de arestas\n",
    "print('Arestas:', len(G_reply.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando a rede de retweets\n",
    "\n",
    "A visualização de redes de retweets é uma importante etapa da análise exploratória de dados pois nos permite inspecionar visualmente a estrutura da rede, entender se existe algum usuário que tenha influência desproporcional e se há diferentes esferas de conversação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instância de tamanho dos nós\n",
    "sizes = [x[1] for x in G_rt.degree()]\n",
    "\n",
    "# Desenhando a rede de retweets!\n",
    "nx.draw_networkx(G_rt, \n",
    "    with_labels = False, \n",
    "    node_size = sizes,\n",
    "    width = 0.1, alpha = 0.5,\n",
    "    arrowsize = 2, linewidths = 0)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralidade de grau\n",
    "\n",
    "Centralidade de grau é uma métrica de importância de um nó para uma rede. Para redes direcionadas como as do Twitter, precisamos ter o cuidado de distinguir entre centralidade de grau de entrada e de saída, especialmente em redes de retweets. A centralidade de grau de entrada para redes de retweets sinaliza usuários que estão recebendo muitos retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralidade de entrada para a rede de retweets \n",
    "rt_centrality = nx.in_degree_centrality(G_rt)\n",
    "\n",
    "# Centralidade de entrada para a rede de respostas \n",
    "reply_centrality = nx.in_degree_centrality(G_reply)\n",
    "\n",
    "# Armazenando as centralidades em um data frame\n",
    "column_names = ['screen_name', 'degree_centrality']\n",
    "\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Visualizando os cinco primeiros nós com maior centralidade de grau na rede de retweets\n",
    "print(rt.sort_values('degree_centrality', ascending = False).head())\n",
    "\n",
    "# Visualizando os cinco primeiros nós com maior centralidade de grau na rede de respostas\n",
    "print(reply.sort_values('degree_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralidade de proximidade\n",
    "\n",
    "A centralidade de proximidade para as redes de retweets e de respostas sinaliza que há usuários que fazem a ponte entre diferentes comunidades do Twitter. Essas comunidades podem estar ligadas por tópicos de interesse ou ideologia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralidade de proximidade para a rede de retweets \n",
    "rt_centrality = nx.betweenness_centrality(G_rt)\n",
    "\n",
    "# Centralidade de proximidade para a rede de respostas \n",
    "reply_centrality = nx.betweenness_centrality(G_reply)\n",
    "\n",
    "# Armazenando as centralidades em um data frame\n",
    "column_names = ['screen_name', 'betweenness_centrality']\n",
    "\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Visualizando os cinco primeiros nós com maior centralidade de proximidade na rede de retweets\n",
    "print(rt.sort_values('betweenness_centrality', ascending = False).head())\n",
    "\n",
    "# Visualizando os cinco primeiros nós com maior centralidade de proximidade na rede de respostas\n",
    "print(reply.sort_values('betweenness_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio\n",
    "\n",
    "\"The Ratio\", como é chamada, é uma medida de rede específica do Twitter e normalmente é usada para julgar a impopularidade de um tweet. É calculado tomando o número de respostas e dividindo pelo número de retweets, no caso desse estudo será utilizada a centralidade de grau de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['screen_name', 'degree']\n",
    "\n",
    "degree_rt = pd.DataFrame(list(G_rt.in_degree()), columns = column_names)\n",
    "degree_reply = pd.DataFrame(list(G_reply.in_degree()), columns = column_names)\n",
    "\n",
    "# Juntando os dois data frames\n",
    "ratio = degree_rt.merge(degree_reply, on = 'screen_name', suffixes = ('_rt', '_reply'))\n",
    "\n",
    "# Calculando o ratio\n",
    "ratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']\n",
    "\n",
    "# Excluindo os tweets com menos de 5 retweets\n",
    "ratio = ratio[ratio['degree_rt'] >= 5]\n",
    "\n",
    "# Vizualizando os cinco maiores ratio\n",
    "print(ratio.sort_values('ratio', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências bibliográficas\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
